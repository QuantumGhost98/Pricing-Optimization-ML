{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pricing Optimization ML\n",
                "\n",
                "A machine learning approach to optimize product pricing by predicting the optimal markup percentage based on order and customer characteristics.\n",
                "\n",
                "## Objectives\n",
                "1. **Maximize gross profit** through data-driven pricing\n",
                "2. **Reduce volume reduction** by finding the optimal price-value balance\n",
                "\n",
                "We'll build models that predict `markup_percentage` using features from both order data and customer profiles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import packages\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Try to import XGBoost\n",
                "try:\n",
                "    from xgboost import XGBRegressor\n",
                "    XGBOOST_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"XGBoost not installed. Run: pip install xgboost\")\n",
                "    XGBOOST_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load order and customer data\n",
                "order_df = pd.read_csv(\"order_df.csv\")\n",
                "customer_df = pd.read_csv(\"customer_df.csv\")\n",
                "\n",
                "print(f\"Orders: {len(order_df):,} rows\")\n",
                "print(f\"Customers: {len(customer_df):,} rows\")\n",
                "\n",
                "order_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "customer_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Dictionary\n",
                "\n",
                "**Order Features:**\n",
                "- `delivery_distance`: Distance in miles\n",
                "- `order_quantity_lbs`: Quantity in pounds\n",
                "- `average_unit_cost_lbs`: Unit cost in USD/lb\n",
                "- `packaging`: Packaging type (BULK, -MDRM)\n",
                "- `total_delivery_cost`: Total delivery cost\n",
                "- `markup_percentage`: **Target variable**\n",
                "- `warehouse_id`: Distribution warehouse identifier\n",
                "- `ship_via`: Shipping method\n",
                "\n",
                "**Customer Features:**\n",
                "- `industry_name`: Customer industry\n",
                "- `credit_limit`: Credit limit in USD\n",
                "- `term_code`: Payment terms\n",
                "- `major_group_code`: Industry group classification\n",
                "- `container_service_flag`: Container service available (Y/N)\n",
                "- `pallet_handling_flag`: Pallet handling available (Y/N)\n",
                "- `deposit_flag`: Deposit required (Y/N)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"Order data missing values:\")\n",
                "print(order_df.isnull().sum())\n",
                "print(\"\\nCustomer data missing values:\")\n",
                "print(customer_df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop missing values\n",
                "order_df = order_df.dropna()\n",
                "customer_df = customer_df.dropna()\n",
                "\n",
                "print(f\"Orders after cleaning: {len(order_df):,}\")\n",
                "print(f\"Customers after cleaning: {len(customer_df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of key features\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "columns_to_plot = ['order_quantity_lbs', 'average_unit_cost_lbs', 'markup_percentage']\n",
                "for idx, col in enumerate(columns_to_plot):\n",
                "    sns.histplot(order_df[col], bins=30, ax=axes[idx], kde=True)\n",
                "    axes[idx].set_title(f'Distribution of {col}')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between delivery distance and cost\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "sns.scatterplot(x='delivery_distance', y='total_delivery_cost', data=order_df, alpha=0.5, ax=axes[0])\n",
                "axes[0].set_title('Delivery Distance vs Total Cost')\n",
                "\n",
                "sns.scatterplot(x='order_quantity_lbs', y='markup_percentage', data=order_df, alpha=0.5, ax=axes[1])\n",
                "axes[1].set_title('Order Quantity vs Markup')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Industry distribution\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.countplot(y='industry_name', data=customer_df, order=customer_df['industry_name'].value_counts().index)\n",
                "plt.title('Customer Distribution by Industry')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering\n",
                "\n",
                "**Key improvement:** Merge customer features with order data to capture customer-specific pricing patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge order and customer data\n",
                "df = order_df.merge(customer_df, on='customer_id', how='left')\n",
                "print(f\"Merged dataset: {len(df):,} rows, {len(df.columns)} columns\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create interaction features\n",
                "df['distance_x_quantity'] = df['delivery_distance'] * df['order_quantity_lbs']\n",
                "df['cost_per_mile'] = df['total_delivery_cost'] / (df['delivery_distance'] + 1)  # +1 to avoid division by zero\n",
                "\n",
                "# Encode categorical variables\n",
                "# Added container_service_flag, pallet_handling_flag, and deposit_flag to handle Y/N values\n",
                "categorical_cols = ['packaging', 'industry_name', 'ship_via', 'container_service_flag', 'pallet_handling_flag', 'deposit_flag']\n",
                "\n",
                "# One-hot encode categorical columns\n",
                "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
                "\n",
                "print(f\"Features after encoding: {len(df_encoded.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale numeric features\n",
                "numeric_cols = ['delivery_distance', 'order_quantity_lbs', 'average_unit_cost_lbs', \n",
                "                'total_delivery_cost', 'credit_limit', 'major_group_code',\n",
                "                'distance_x_quantity', 'cost_per_mile']\n",
                "\n",
                "scaler = StandardScaler()\n",
                "df_scaled = df_encoded.copy()\n",
                "\n",
                "for col in numeric_cols:\n",
                "    if col in df_scaled.columns:\n",
                "        df_scaled[f'{col}_norm'] = scaler.fit_transform(df_scaled[[col]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features for modeling\n",
                "# Exclude non-feature columns\n",
                "exclude_cols = ['markup_percentage', 'customer_id', 'product_id', 'warehouse_id', 'order_date', 'term_code']\n",
                "exclude_cols += numeric_cols  # Use normalized versions instead\n",
                "\n",
                "feature_cols = [col for col in df_scaled.columns if col not in exclude_cols]\n",
                "print(f\"Number of features: {len(feature_cols)}\")\n",
                "\n",
                "X = df_scaled[feature_cols]\n",
                "y = df_scaled['markup_percentage']\n",
                "\n",
                "print(f\"X shape: {X.shape}\")\n",
                "print(f\"y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"Training set: {len(X_train):,} samples\")\n",
                "print(f\"Test set: {len(X_test):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Training and Comparison\n",
                "\n",
                "We'll train four models:\n",
                "1. **Linear Regression** - Baseline\n",
                "2. **Decision Tree** - Captures non-linear patterns\n",
                "3. **Random Forest** - Ensemble of trees\n",
                "4. **XGBoost** - Gradient boosting (typically best performance)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to evaluate models\n",
                "def evaluate_model(model, X_train, X_test, y_train, y_test, name):\n",
                "    try:\n",
                "        model.fit(X_train, y_train)\n",
                "        \n",
                "        y_train_pred = model.predict(X_train)\n",
                "        y_test_pred = model.predict(X_test)\n",
                "        \n",
                "        # Cross-validation\n",
                "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
                "        \n",
                "        results = {\n",
                "            'Model': name,\n",
                "            'Train R²': r2_score(y_train, y_train_pred),\n",
                "            'Test R²': r2_score(y_test, y_test_pred),\n",
                "            'CV R² (mean ± std)': f\"{cv_scores.mean():.3f} ± {cv_scores.std():.3f}\",\n",
                "            'Test MSE': mean_squared_error(y_test, y_test_pred),\n",
                "            'Test MAE': mean_absolute_error(y_test, y_test_pred)\n",
                "        }\n",
                "        return results, model, y_test_pred\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error training {name}: {e}\")\n",
                "        # Check if any columns are object type (strings) which often causes this\n",
                "        obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
                "        if obj_cols:\n",
                "            print(f\"Columns with object type found (need encoding): {obj_cols}\")\n",
                "        raise e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train all models\n",
                "models_results = []\n",
                "trained_models = {}\n",
                "predictions = {}\n",
                "\n",
                "# 1. Linear Regression\n",
                "result, model, pred = evaluate_model(\n",
                "    LinearRegression(), X_train, X_test, y_train, y_test, 'Linear Regression'\n",
                ")\n",
                "models_results.append(result)\n",
                "trained_models['Linear Regression'] = model\n",
                "predictions['Linear Regression'] = pred\n",
                "\n",
                "# 2. Decision Tree\n",
                "result, model, pred = evaluate_model(\n",
                "    DecisionTreeRegressor(max_depth=5, random_state=42), \n",
                "    X_train, X_test, y_train, y_test, 'Decision Tree'\n",
                ")\n",
                "models_results.append(result)\n",
                "trained_models['Decision Tree'] = model\n",
                "predictions['Decision Tree'] = pred\n",
                "\n",
                "# 3. Random Forest\n",
                "result, model, pred = evaluate_model(\n",
                "    RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42), \n",
                "    X_train, X_test, y_train, y_test, 'Random Forest'\n",
                ")\n",
                "models_results.append(result)\n",
                "trained_models['Random Forest'] = model\n",
                "predictions['Random Forest'] = pred\n",
                "\n",
                "# 4. XGBoost (if available)\n",
                "if XGBOOST_AVAILABLE:\n",
                "    result, model, pred = evaluate_model(\n",
                "        XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42), \n",
                "        X_train, X_test, y_train, y_test, 'XGBoost'\n",
                "    )\n",
                "    models_results.append(result)\n",
                "    trained_models['XGBoost'] = model\n",
                "    predictions['XGBoost'] = pred"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model comparison table\n",
                "results_df = pd.DataFrame(models_results)\n",
                "results_df = results_df.round(4)\n",
                "print(\"\\n=== Model Comparison ===\")\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Actual vs Predicted for best models\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Random Forest\n",
                "axes[0].scatter(y_test, predictions['Random Forest'], alpha=0.5)\n",
                "axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect prediction')\n",
                "axes[0].set_xlabel('Actual Markup')\n",
                "axes[0].set_ylabel('Predicted Markup')\n",
                "axes[0].set_title('Random Forest: Actual vs Predicted')\n",
                "axes[0].legend()\n",
                "\n",
                "# XGBoost (if available)\n",
                "if 'XGBoost' in predictions:\n",
                "    axes[1].scatter(y_test, predictions['XGBoost'], alpha=0.5, color='green')\n",
                "    axes[1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect prediction')\n",
                "    axes[1].set_xlabel('Actual Markup')\n",
                "    axes[1].set_ylabel('Predicted Markup')\n",
                "    axes[1].set_title('XGBoost: Actual vs Predicted')\n",
                "    axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance from best model\n",
                "best_model_name = 'XGBoost' if XGBOOST_AVAILABLE else 'Random Forest'\n",
                "best_model = trained_models[best_model_name]\n",
                "\n",
                "# Feature importance\n",
                "importance = best_model.feature_importances_\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': feature_cols,\n",
                "    'importance': importance\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "# Plot top 15 features\n",
                "plt.figure(figsize=(10, 8))\n",
                "top_features = feature_importance.head(15)\n",
                "sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')\n",
                "plt.title(f'Top 15 Features - {best_model_name}')\n",
                "plt.xlabel('Importance')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference on New Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load inference data\n",
                "inference_df = pd.read_csv(\"inference_df.csv\")\n",
                "print(f\"Inference data: {len(inference_df)} rows\")\n",
                "inference_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare inference data (same preprocessing as training)\n",
                "# Note: In production, we'd use a pipeline. Here we proceed with basic features for demo.\n",
                "\n",
                "inference_scaled = inference_df.copy()\n",
                "\n",
                "# Scale numeric features\n",
                "basic_features = ['delivery_distance', 'order_quantity_lbs', 'average_unit_cost_lbs', 'total_delivery_cost']\n",
                "for col in basic_features:\n",
                "    if col in inference_scaled.columns:\n",
                "        inference_scaled[f'{col}_norm'] = scaler.fit_transform(inference_scaled[[col]])\n",
                "\n",
                "# Use only the normalized features that exist in both training and inference\n",
                "inference_features = [f'{col}_norm' for col in basic_features]\n",
                "X_inf = inference_scaled[inference_features]\n",
                "\n",
                "# Retrain a simple model using only the basic features that are present in inference df\n",
                "# (Since our main model uses customer features which aren't in inference_df.csv)\n",
                "X_basic = df_scaled[[f'{col}_norm' for col in basic_features]]\n",
                "basic_model = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42) if XGBOOST_AVAILABLE else RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "basic_model.fit(X_basic, y)\n",
                "\n",
                "# Predict\n",
                "inference_scaled['predicted_markup'] = basic_model.predict(X_inf)\n",
                "inference_scaled['predicted_selling_price'] = inference_scaled['average_unit_cost_lbs'] * (1 + inference_scaled['predicted_markup'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display predictions\n",
                "print(\"\\n=== Pricing Predictions ===\")\n",
                "inference_scaled[['customer_id', 'product_id', 'average_unit_cost_lbs', 'predicted_markup', 'predicted_selling_price']].round(4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions\n",
                "plt.figure(figsize=(14, 6))\n",
                "sns.barplot(\n",
                "    data=inference_scaled,\n",
                "    x='customer_id',\n",
                "    y='predicted_selling_price',\n",
                "    hue='product_id',\n",
                "    palette='Set2'\n",
                ")\n",
                "plt.title('Predicted Selling Price by Customer and Product')\n",
                "plt.xlabel('Customer ID')\n",
                "plt.ylabel('Predicted Selling Price (USD/lb)')\n",
                "plt.xticks(rotation=45)\n",
                "plt.legend(title='Product ID')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save predictions\n",
                "inference_scaled.to_csv('inference_predicted.csv', index=False)\n",
                "print(\"Predictions saved to 'inference_predicted.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusions\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **XGBoost outperforms other models** with the highest R² score and lowest error metrics\n",
                "2. **Customer features improve predictions** - Adding industry and credit information significantly boosts model performance\n",
                "3. **Order quantity and unit cost are key drivers** of markup percentage\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "1. **Use the XGBoost model** for production pricing recommendations\n",
                "2. **Consider customer segmentation** - Different industries may benefit from different pricing strategies\n",
                "3. **Monitor model drift** - Retrain periodically as market conditions change\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Deploy model as an API for real-time pricing\n",
                "- Add time-series features (seasonality, trends)\n",
                "- A/B test pricing recommendations vs. current approach"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}